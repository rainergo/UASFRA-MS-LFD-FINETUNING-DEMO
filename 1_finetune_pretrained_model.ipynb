{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## I. FINETUNING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 1. Import packages\n",
    "`pip install transformers torch datasets numpy pandas`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# settings.py\n",
    "from settings import dataset_name, model_name, finetuned_model_name, finetuned_models_folder\n",
    "from functions import tokenize, get_tokenizer, get_metrics, plot_confusion_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2. Load dataset \"emotion\" - use train and validation subset\n",
    "```\n",
    "emotion => Labelled Twitter text messages classified into 6 different sentiments:\n",
    "    1. 'sadness'\n",
    "    2. 'anger',\n",
    "    3. 'love',\n",
    "    4. 'surprise'\n",
    "    5. 'fear'\n",
    "    6. 'joy'\n",
    "(On the first download, this may take a while)\n",
    "```\n",
    "##### We use the train subset to finetune the generic BERT to our special needs, here to classify Twitter msgs. You could use any other labeled text dataset for classification according to your needs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name)\n",
    "labels = pd.Series(list(dataset.data['train'].columns[2])).unique().astype(str).tolist()\n",
    "print('Labels of \"emotion\" train dataset:', labels)\n",
    "print('Subsets of \"emotion\" dataset: ', list(dataset.data.keys()))\n",
    "print(f'Size of \"emotion\" train/validation datset: {len(dataset.data[\"train\"])}/{len(dataset.data[\"validation\"])}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('\"\"\"  Example: \"\"\"')\n",
    "index = 42\n",
    "print(f\"TEXT: '{dataset['train']['text'][index]}'\")\n",
    "print(f\"LABEL: {dataset['train']['label'][index]}\")\n",
    "print(f\"LABEL_TEXT: {dataset['train']['label_text'][index]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3. Tokenize text (train and validation subsets)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "emotions_train_tokenized = dataset['train'].map(tokenize, fn_kwargs={'model_name': model_name}, batched=True, batch_size=None)\n",
    "emotions_validation_tokenized = dataset['validation'].map(tokenize, fn_kwargs={'model_name': model_name}, batched=True, batch_size=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4. Now finetune pretrained BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GPU/Cuda (if installed on your computer) or simple CPU?\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device} is used to finetune model!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### \"AutoModelForSequenceClassification\" replaces the head (last linear layer) of the pretrained model with a randomly initialized linear layer. The number of neurons in this new linear layer must correspond to the number of classes/labels in our dataset (here: the number of sentiments = 6). Only this new linear layer will be trained during finetuning, the other parts of the pretrained model will be frozen:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_labels = len(labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name, num_labels=num_labels).to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Settings - check docs for further details: https://huggingface.co/docs/transformers/training\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "logging_steps = len(emotions_train_tokenized) // batch_size\n",
    "finetuned_model_name_and_path = f\"./{finetuned_models_folder}/{finetuned_model_name}\"\n",
    "tokenizer = get_tokenizer(model_name=model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training Arguments - check docs for further details: https://huggingface.co/docs/transformers/training\n",
    "training_args = TrainingArguments(output_dir=finetuned_model_name_and_path,\n",
    "                                  num_train_epochs=num_epochs,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=weight_decay,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=False,\n",
    "                                  log_level=\"error\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trainer Arguments - check docs for further details: https://huggingface.co/docs/transformers/training\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=get_metrics,\n",
    "                  train_dataset=emotions_train_tokenized,\n",
    "                  eval_dataset=emotions_validation_tokenized,\n",
    "                  tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### FINETUNING STARTS:\n",
    "###### WARNING: This takes some time (~ 3 to 30 mins or more) depending on \"settings\" above and your GPU/CPU/hardware setup:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start finetuning / train last linear layer only:\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Metrics (for validation dataset) for model that was just finetuned :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validation_set_predictions = trainer.predict(emotions_validation_tokenized)\n",
    "print('General metrics for the finetuned model: ', validation_set_predictions.metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = np.argmax(validation_set_predictions.predictions, axis=1)\n",
    "print('Further metrics for finetuned model: Confusion Matrix')\n",
    "plot_confusion_matrix(y_pred, list(dataset['validation']['label']), labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 5. Save finetuned model for later predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir=finetuned_model_name_and_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 6. Clear memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Empty cache\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
